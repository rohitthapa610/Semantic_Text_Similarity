{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be7ba3c7",
   "metadata": {},
   "source": [
    "# Semantic Textual Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e7fdc",
   "metadata": {},
   "source": [
    "\n",
    "# Problem Statement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07c162b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The data contains a pair of paragraphs. These text paragraphs are randomly sampled from a raw dataset. Each pair of       sentences may or may not be semantically similar. The candidate is to predict a value between 0-1 indicating the similarity between the pair of text. A sample of a similar dataseet will be used as test data, therefore it's crucial to the model solution using provided dataset. Build an algorithm/model that can quantify the degree of similarity between the two text-based on Semantic similarity. Semantic Textual Similarity (STS) assesses the degree to which two sentences are semantically equivalent to each other. 1 means highly similar 0 means highly dissimilar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c89e7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Import some library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125435f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer  # For Lemmetization of words\n",
    "from nltk.corpus import stopwords  # Load the list of stopwords\n",
    "from nltk import word_tokenize # Convert paragraph in tokens\n",
    "\n",
    "from gensim.models import word2vec # represent words in vectors\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b733d136",
   "metadata": {},
   "source": [
    "# Read Data-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b7a3cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of text_data :  (3000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>broadband challenges tv viewing the number of ...</td>\n",
       "      <td>gardener wins double in glasgow britain s jaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rap boss arrested over drug find rap mogul mar...</td>\n",
       "      <td>amnesty chief laments war failure the lack of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>player burn-out worries robinson england coach...</td>\n",
       "      <td>hanks greeted at wintry premiere hollywood sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>hearts of oak 3-2 cotonsport hearts of oak set...</td>\n",
       "      <td>redford s vision of sundance despite sporting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sir paul rocks super bowl crowds sir paul mcca...</td>\n",
       "      <td>mauresmo opens with victory in la amelie maure...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID                                              text1  \\\n",
       "0          0  broadband challenges tv viewing the number of ...   \n",
       "1          1  rap boss arrested over drug find rap mogul mar...   \n",
       "2          2  player burn-out worries robinson england coach...   \n",
       "3          3  hearts of oak 3-2 cotonsport hearts of oak set...   \n",
       "4          4  sir paul rocks super bowl crowds sir paul mcca...   \n",
       "\n",
       "                                               text2  \n",
       "0  gardener wins double in glasgow britain s jaso...  \n",
       "1  amnesty chief laments war failure the lack of ...  \n",
       "2  hanks greeted at wintry premiere hollywood sta...  \n",
       "3  redford s vision of sundance despite sporting ...  \n",
       "4  mauresmo opens with victory in la amelie maure...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the given data-set using pandas\n",
    "text_data = pd.read_csv(\"Precily_Text_Similarity.csv\")\n",
    "print(\"shape of text_data : \", text_data.shape)\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ece9b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unique_ID    0\n",
       "text1        0\n",
       "text2        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data.isnull().sum() # Check if text data have any null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b29fceb",
   "metadata": {},
   "source": [
    "# Preprocessing of text1 & text2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7199a255",
   "metadata": {},
   "source": [
    "* Removing punctuations like .,!$()*%@\n",
    "\n",
    "* Removing stop words\n",
    "\n",
    "* Lower casing\n",
    "\n",
    "* Tokenization\n",
    "\n",
    "* Stemming\n",
    "\n",
    "* Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c6d0194",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01278c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 3000/3000 [05:30<00:00,  9.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Combining all the above statements\n",
    "\n",
    "preprocessed_text1 = []\n",
    "\n",
    "# tqdm is for printing the status bar\n",
    "\n",
    "for sentance in tqdm(text_data['text1'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "\n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))\n",
    "    preprocessed_text1.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be6cef36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>broadband challenges tv viewing number europea...</td>\n",
       "      <td>gardener wins double in glasgow britain s jaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rap boss arrested drug find rap mogul marion s...</td>\n",
       "      <td>amnesty chief laments war failure the lack of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>player burn worries robinson england coach and...</td>\n",
       "      <td>hanks greeted at wintry premiere hollywood sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>hearts oak 3 2 cotonsport hearts oak set ghana...</td>\n",
       "      <td>redford s vision of sundance despite sporting ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sir paul rocks super bowl crowds sir paul mcca...</td>\n",
       "      <td>mauresmo opens with victory in la amelie maure...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID                                              text1  \\\n",
       "0          0  broadband challenges tv viewing number europea...   \n",
       "1          1  rap boss arrested drug find rap mogul marion s...   \n",
       "2          2  player burn worries robinson england coach and...   \n",
       "3          3  hearts oak 3 2 cotonsport hearts oak set ghana...   \n",
       "4          4  sir paul rocks super bowl crowds sir paul mcca...   \n",
       "\n",
       "                                               text2  \n",
       "0  gardener wins double in glasgow britain s jaso...  \n",
       "1  amnesty chief laments war failure the lack of ...  \n",
       "2  hanks greeted at wintry premiere hollywood sta...  \n",
       "3  redford s vision of sundance despite sporting ...  \n",
       "4  mauresmo opens with victory in la amelie maure...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging preprocessed_text1 in text_data\n",
    "\n",
    "text_data['text1'] = preprocessed_text1\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca404868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 3000/3000 [05:20<00:00,  9.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# combining all the above stundents \n",
    "preprocessed_text2 = []\n",
    "\n",
    "# tqdm is for printing the status bar\n",
    "\n",
    "for sentance in tqdm(text_data['text2'].values):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "   \n",
    "    sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))\n",
    "    preprocessed_text2.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6636c4e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>broadband challenges tv viewing number europea...</td>\n",
       "      <td>gardener wins double glasgow britain jason gar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>rap boss arrested drug find rap mogul marion s...</td>\n",
       "      <td>amnesty chief laments war failure lack public ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>player burn worries robinson england coach and...</td>\n",
       "      <td>hanks greeted wintry premiere hollywood star t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>hearts oak 3 2 cotonsport hearts oak set ghana...</td>\n",
       "      <td>redford vision sundance despite sporting cordu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>sir paul rocks super bowl crowds sir paul mcca...</td>\n",
       "      <td>mauresmo opens victory la amelie mauresmo mari...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID                                              text1  \\\n",
       "0          0  broadband challenges tv viewing number europea...   \n",
       "1          1  rap boss arrested drug find rap mogul marion s...   \n",
       "2          2  player burn worries robinson england coach and...   \n",
       "3          3  hearts oak 3 2 cotonsport hearts oak set ghana...   \n",
       "4          4  sir paul rocks super bowl crowds sir paul mcca...   \n",
       "\n",
       "                                               text2  \n",
       "0  gardener wins double glasgow britain jason gar...  \n",
       "1  amnesty chief laments war failure lack public ...  \n",
       "2  hanks greeted wintry premiere hollywood star t...  \n",
       "3  redford vision sundance despite sporting cordu...  \n",
       "4  mauresmo opens victory la amelie mauresmo mari...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging preprocessed_text2 in text_data\n",
    "\n",
    "text_data['text2'] = preprocessed_text2\n",
    "text_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "776f229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenizer(text):\n",
    "            tokens = word_tokenize(text) #tokenize the text\n",
    "            lemmatizer = WordNetLemmatizer() #stems the text\n",
    "            tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "            return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827e2840",
   "metadata": {},
   "source": [
    "# Proposed Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7b796",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Word Embedding Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c04715",
   "metadata": {},
   "source": [
    "* Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Pre-trained word embeddings are also available in the word2vec code.google page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b191f26f",
   "metadata": {},
   "source": [
    "*  In this I am using Google news pre trained vectors and compare similarity between text1 & text2 using n_similarity method in gensim library which is nothing compares cosine similarity between two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09281b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pre_trained Google News Vectors \n",
    "\n",
    "wordmodelfile=\"GoogleNews-vectors-negative300.bin.gz\"\n",
    "wordmodel= gensim.models.KeyedVectors.load_word2vec_format(wordmodelfile, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26f6f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code check if word in text1 & text2 present in our google news vectors vocabalry.\n",
    "# if not it removes that word and if present it compares similarity score between text1 and text2 words\n",
    "\n",
    "\n",
    "similarity = [] # List for store similarity score\n",
    "\n",
    "for ind in text_data.index:\n",
    "    \n",
    "        s1 = text_data['text1'][ind]\n",
    "        s2 = text_data['text2'][ind]\n",
    "        \n",
    "        if s1==s2:\n",
    "                 similarity.append(0.0) # 0 means highly similar\n",
    "                \n",
    "        else:   \n",
    "\n",
    "            s1words = word_tokenizer(s1)\n",
    "            s2words = word_tokenizer(s2)\n",
    "            \n",
    "           \n",
    "            vocab = wordmodel.key_to_index  #the vocabulary considered in the word embeddings\n",
    "            \n",
    "            if len(s1words and s2words)==0:\n",
    "                    similarity.append(1.0)\n",
    "\n",
    "            else:\n",
    "                \n",
    "                for word in s1words.copy(): #remove sentence words not found in the vocab\n",
    "                    if (word not in vocab):\n",
    "                           \n",
    "                            \n",
    "                            s1words.remove(word)\n",
    "                        \n",
    "                    \n",
    "                for word in s2words.copy(): #idem\n",
    "\n",
    "                    if (word not in vocab):\n",
    "                           \n",
    "                            s2words.remove(word)\n",
    "                            \n",
    "                            \n",
    "                similarity.append((1-wordmodel.n_similarity(s1words, s2words))) # as it is given 1 means highly dissimilar & 0 means highly similar\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b93789",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Final Submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8c3a9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "• Make Final DataFrame and save a CSV file of similarity scores with Unique_ID.(Columns : Unique_ID, Similarity_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad162fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>Similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.314564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.366671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.282510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.383152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.207984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unique_ID  Similarity_score\n",
       "0          0          0.314564\n",
       "1          1          0.366671\n",
       "2          2          0.282510\n",
       "3          3          0.383152\n",
       "4          4          0.207984"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Unique_ID and similarity\n",
    "\n",
    "final_score = pd.DataFrame({'Unique_ID':text_data.Unique_ID, 'Similarity_score':similarity})\n",
    "final_score.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d84c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df as CSV file\n",
    "\n",
    "final_score.to_csv('final_score.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8adb336",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
